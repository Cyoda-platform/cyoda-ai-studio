"""
OpenAI Assistant Wrapper for Cyoda AI Assistant.

Provides a wrapper around OpenAI Agents SDK with Cyoda session persistence,
similar to CyodaAssistantWrapper for Google ADK.
"""

import asyncio
import importlib
import logging
from typing import Any, AsyncGenerator, Optional

# Import OpenAI agents module directly to avoid namespace collision with local 'agents' package
_openai_agents = importlib.import_module("agents")
Agent = _openai_agents.Agent
Runner = _openai_agents.Runner

from application.entity.conversation import Conversation
from application.services.openai_agents_service import OpenAIAgentsService
from common.service.service import EntityServiceError

logger = logging.getLogger(__name__)


class OpenAIAssistantWrapper:
    """
    Wrapper for OpenAI Agent with Cyoda session persistence.

    Manages:
    - Session state persistence to Cyoda Conversation.workflow_cache
    - Agent lifecycle
    - Message processing and response extraction
    """

    def __init__(self, agent: Agent, entity_service: Any):
        """
        Initialize the wrapper.

        Args:
            agent: The OpenAI Agent
            entity_service: Cyoda entity service for persistence
        """
        self.agent = agent
        self.entity_service = entity_service
        self.service = OpenAIAgentsService()

        logger.info(f"OpenAIAssistantWrapper initialized with agent: {agent.name}")

    async def process_message(
        self,
        user_message: str,
        conversation_history: list[dict[str, str]],
        conversation_id: str | None = None,
        user_id: str = "guest.anonymous",
    ) -> dict[str, Any]:
        """
        Process a user message using the OpenAI agent with persistent session.

        Args:
            user_message: User's message/question
            conversation_history: Previous messages in the conversation
            conversation_id: Cyoda Conversation technical ID (for persistence)
            user_id: User ID from authentication

        Returns:
            Dictionary with:
                - response: Agent's text response
                - conversation_id: Cyoda conversation ID (for persistence)
                - updated_history: Updated conversation history
                - hooks: Any UI hooks generated by tools
        """
        try:
            logger.info(
                f"Processing message for user {user_id}, "
                f"conversation_id={conversation_id}, "
                f"history_length={len(conversation_history)}"
            )

            # Build full prompt with conversation history
            full_prompt = self._build_prompt(user_message, conversation_history)

            # Create context dict for tools to store hooks
            context = {
                "conversation_id": conversation_id,
                "user_id": user_id,
            }

            # Run the agent
            logger.debug(f"Running agent: {self.agent.name}")
            result = await Runner.run(self.agent, full_prompt, context=context)

            # Extract response
            response_text = result.final_output or ""
            logger.debug(f"Agent response length: {len(response_text)} characters")

            # Update conversation history
            updated_history = conversation_history.copy()
            updated_history.append({"role": "user", "content": user_message})
            updated_history.append({"role": "assistant", "content": response_text})

            # Extract hooks from result context if available
            hooks = self._extract_hooks_from_result(result)
            if hooks:
                logger.info(f"Found {len(hooks)} hook(s) in result")

            # Persist to Cyoda if conversation_id provided
            if conversation_id:
                await self._persist_conversation(
                    conversation_id, updated_history, user_id
                )

            return {
                "response": response_text,
                "conversation_id": conversation_id,
                "updated_history": updated_history,
                "hooks": hooks,
            }

        except Exception as e:
            logger.exception(f"Error processing message: {e}")
            raise

    async def stream_message(
        self,
        user_message: str,
        conversation_history: list[dict[str, str]],
        conversation_id: str | None = None,
        user_id: str = "guest.anonymous",
    ) -> AsyncGenerator[str, None]:
        """
        Stream a message response from the agent with real-time chunks.

        Args:
            user_message: User's message/question
            conversation_history: Previous messages in the conversation
            conversation_id: Cyoda Conversation technical ID
            user_id: User ID from authentication

        Yields:
            Content chunks as they become available, or hook JSON when hooks are available
        """
        try:
            logger.info(
                f"Streaming message for user {user_id}, "
                f"conversation_id={conversation_id}"
            )

            # Build full prompt
            full_prompt = self._build_prompt(user_message, conversation_history)

            # Create context dict for tools to store hooks
            context = {
                "conversation_id": conversation_id,
                "user_id": user_id,
            }

            # Run agent with streaming to get real-time events
            # Note: run_streamed() returns RunResultStreaming directly, not an awaitable
            result = Runner.run_streamed(self.agent, full_prompt, context=context)

            accumulated_content = ""

            # Stream events from the agent
            async for event in result.stream_events():
                logger.debug(f"Streaming event type: {event.type}")

                # Handle raw response events (text deltas from LLM)
                if event.type == "raw_response_event":
                    # Raw response events contain streaming text deltas
                    if hasattr(event.data, 'delta'):
                        content = event.data.delta
                        if content:
                            logger.debug(f"Raw response delta: {content[:50]}")
                            accumulated_content += content
                            yield content

                # Handle run item events (completed messages, tool calls, etc.)
                elif event.type == "run_item_stream_event":
                    logger.debug(f"Run item type: {event.item.type if hasattr(event, 'item') else 'unknown'}")
                    # For message output items, extract the text content
                    if hasattr(event, 'item') and event.item.type == "message_output_item":
                        if hasattr(event.item, 'content'):
                            # Content is a list of content blocks
                            for content_block in event.item.content:
                                if hasattr(content_block, 'text'):
                                    text = content_block.text
                                    if text and text not in accumulated_content:
                                        logger.debug(f"Message output: {text[:50]}")
                                        accumulated_content += text
                                        yield text
                    # For tool call items, log them but don't yield
                    elif hasattr(event, 'item') and event.item.type == "tool_call_item":
                        logger.info(f"Tool call detected: {event.item.name if hasattr(event.item, 'name') else 'unknown'}")

                # Handle agent updated events (handoffs to sub-agents)
                elif event.type == "agent_updated_stream_event":
                    logger.info(f"Agent handoff detected: {event.new_agent.name}")
                    # Continue streaming - the loop will capture events from the new agent

            # After streaming completes, check for final output
            # This ensures we capture the complete response including after handoffs
            if result.final_output:
                final_output = str(result.final_output)
                logger.debug(f"Final output: {final_output[:100]}")
                if final_output and final_output not in accumulated_content:
                    logger.debug(f"Adding final output: {final_output[:100]}...")
                    accumulated_content += final_output
                    yield final_output

            # Extract hooks from result and yield them
            hooks = self._extract_hooks_from_result(result)
            if hooks:
                logger.info(f"Found {len(hooks)} hook(s) in result, yielding as JSON")
                import json
                for hook in hooks:
                    yield json.dumps({"__hook__": hook})

            # Persist after streaming completes
            if conversation_id:
                updated_history = conversation_history.copy()
                updated_history.append({"role": "user", "content": user_message})
                updated_history.append(
                    {"role": "assistant", "content": accumulated_content}
                )
                await self._persist_conversation(
                    conversation_id, updated_history, user_id
                )

            logger.debug(f"Streaming completed. Total content: {len(accumulated_content)} chars")

        except Exception as e:
            logger.exception(f"Error streaming message: {e}")
            raise

    def _extract_hooks_from_result(self, result: Any) -> list[dict[str, Any]]:
        """
        Extract UI hooks from the agent result.

        Hooks are stored in the result's context by tools during execution.
        This method looks for hooks that were added to the context.

        Args:
            result: The RunResult or RunResultStreaming from the agent

        Returns:
            List of hook dictionaries, or empty list if none found
        """
        hooks = []
        try:
            # OpenAI SDK stores context in context_wrapper.context
            if hasattr(result, 'context_wrapper') and result.context_wrapper:
                context_wrapper = result.context_wrapper
                context = context_wrapper.context

                # Context can be any object, check if it has state or is a dict
                if context:
                    if isinstance(context, dict):
                        # Check for last_tool_hook (single hook)
                        if "last_tool_hook" in context:
                            hook = context["last_tool_hook"]
                            if hook:
                                hooks.append(hook)
                                logger.info(f"Extracted hook from context: {hook.get('type', 'unknown')}")

                        # Check for ui_functions (list of hooks)
                        if "ui_functions" in context:
                            ui_functions = context["ui_functions"]
                            if isinstance(ui_functions, list):
                                hooks.extend(ui_functions)
                                logger.info(f"Extracted {len(ui_functions)} UI functions from context")
                    elif hasattr(context, 'state') and isinstance(context.state, dict):
                        # Context is an object with a state dict (like MockToolContext)
                        state = context.state

                        # Check for last_tool_hook (single hook)
                        if "last_tool_hook" in state:
                            hook = state["last_tool_hook"]
                            if hook:
                                hooks.append(hook)
                                logger.info(f"Extracted hook from context.state: {hook.get('type', 'unknown')}")

                        # Check for ui_functions (list of hooks)
                        if "ui_functions" in state:
                            ui_functions = state["ui_functions"]
                            if isinstance(ui_functions, list):
                                hooks.extend(ui_functions)
                                logger.info(f"Extracted {len(ui_functions)} UI functions from context.state")
        except Exception as e:
            logger.debug(f"Could not extract hooks from result: {e}")

        return hooks

    def _build_prompt(
        self,
        user_message: str,
        conversation_history: list[dict[str, str]],
    ) -> str:
        """
        Build a full prompt from conversation history and current message.

        Args:
            user_message: Current user message
            conversation_history: Previous conversation messages

        Returns:
            Formatted prompt string
        """
        if not conversation_history:
            return user_message

        # Format conversation history
        history_lines = []
        for msg in conversation_history:
            role = msg.get("role", "user").capitalize()
            content = msg.get("content", "")
            history_lines.append(f"{role}: {content}")

        # Add current message
        history_lines.append(f"User: {user_message}")

        return "\n".join(history_lines)

    async def _persist_conversation(
        self,
        conversation_id: str,
        conversation_history: list[dict[str, str]],
        user_id: str,
    ) -> None:
        """
        Persist conversation to Cyoda.

        Args:
            conversation_id: Cyoda Conversation technical ID
            conversation_history: Updated conversation history
            user_id: User ID
        """
        try:
            logger.debug(f"Persisting conversation {conversation_id}")

            # Get the conversation entity
            response = await self.entity_service.get_by_id(
                entity_id=conversation_id,
                entity_class="Conversation",
                entity_version="1",
            )

            if not response:
                logger.warning(f"Conversation {conversation_id} not found")
                return

            # Update workflow_cache with conversation history
            # response.data is already a dict, not a Pydantic model
            conversation_data = response.data if isinstance(response.data, dict) else response.data.model_dump()
            conversation_data["workflow_cache"] = {
                "conversation_history": conversation_history,
                "last_updated": asyncio.get_event_loop().time(),
            }

            # Save updated conversation
            await self.entity_service.update(
                entity_id=conversation_id,
                entity=conversation_data,
                entity_class="Conversation",
                entity_version="1",
            )

            logger.debug(f"Conversation {conversation_id} persisted successfully")

        except EntityServiceError as e:
            logger.error(f"Error persisting conversation: {e}")
            # Don't raise - persistence failure shouldn't break the conversation
        except Exception as e:
            logger.exception(f"Unexpected error persisting conversation: {e}")

