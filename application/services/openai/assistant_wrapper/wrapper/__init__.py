"""OpenAI Assistant Wrapper - Re-exports for backward compatibility."""

import importlib
import logging
from typing import Any, AsyncGenerator

from application.services.openai.agents_service import OpenAIAgentsService

from ..models import StreamingState
from ..constants import (
    EVENT_TYPE_RAW_RESPONSE,
    EVENT_TYPE_RUN_ITEM,
    EVENT_TYPE_AGENT_UPDATED,
)
from ..event_handlers import EventHandlers

from .persistence import persist_conversation, build_prompt
from .message_processing import extract_hooks_from_result, process_message
from .streaming import process_streaming_events, stream_message

logger = logging.getLogger(__name__)

# Import OpenAI agents module directly to avoid namespace collision with local 'agents' package
_openai_agents = importlib.import_module("agents")
Agent = _openai_agents.Agent
Runner = _openai_agents.Runner


class OpenAIAssistantWrapper:
    """Wrapper for OpenAI Agent with Cyoda session persistence.

    Manages:
    - Session state persistence to Cyoda Conversation.workflow_cache
    - Agent lifecycle
    - Message processing and response extraction
    """

    def __init__(self, agent: Agent, entity_service: Any):
        """Initialize the wrapper.

        Args:
            agent: The OpenAI Agent
            entity_service: Cyoda entity service for persistence
        """
        self.agent = agent
        self.entity_service = entity_service
        self.service = OpenAIAgentsService()
        self.event_handlers = EventHandlers()

        logger.info(f"OpenAIAssistantWrapper initialized with agent: {agent.name}")

    def _build_prompt(
        self, user_message: str, conversation_history: list[dict[str, str]]
    ) -> str:
        """Build a full prompt from conversation history and current message."""
        return build_prompt(user_message, conversation_history)

    def _extract_hooks_from_result(self, result: Any) -> list[dict[str, Any]]:
        """Extract UI hooks from the agent result."""
        return extract_hooks_from_result(result)

    async def _persist_conversation(
        self,
        conversation_id: str,
        conversation_history: list[dict[str, str]],
        user_id: str,
    ) -> None:
        """Persist conversation to Cyoda."""
        await persist_conversation(
            self.entity_service, conversation_id, conversation_history, user_id
        )

    async def process_message(
        self,
        user_message: str,
        conversation_history: list[dict[str, str]],
        conversation_id: str | None = None,
        user_id: str = "guest.anonymous",
    ) -> dict[str, Any]:
        """Process a user message using the OpenAI agent with persistent session.

        Args:
            user_message: User's message/question
            conversation_history: Previous messages in the conversation
            conversation_id: Cyoda Conversation technical ID (for persistence)
            user_id: User ID from authentication

        Returns:
            Dictionary with:
                - response: Agent's text response
                - conversation_id: Cyoda conversation ID (for persistence)
                - updated_history: Updated conversation history
                - hooks: Any UI hooks generated by tools
        """
        try:
            logger.info(
                f"Processing message for user {user_id}, "
                f"conversation_id={conversation_id}, "
                f"history_length={len(conversation_history)}"
            )

            # Build full prompt with conversation history
            full_prompt = self._build_prompt(user_message, conversation_history)

            # Create context dict for tools to store hooks
            context = {
                "conversation_id": conversation_id,
                "user_id": user_id,
            }

            # Run the agent
            response_text, hooks = await process_message(
                self.agent, user_message, full_prompt, context
            )

            # Update conversation history
            updated_history = conversation_history.copy()
            updated_history.append({"role": "user", "content": user_message})
            updated_history.append({"role": "assistant", "content": response_text})

            # Persist to Cyoda if conversation_id provided
            if conversation_id:
                await self._persist_conversation(
                    conversation_id, updated_history, user_id
                )

            return {
                "response": response_text,
                "conversation_id": conversation_id,
                "updated_history": updated_history,
                "hooks": hooks,
            }

        except Exception as e:
            logger.exception(f"Error processing message: {e}")
            raise

    async def _process_streaming_events(
        self, result: Any
    ) -> AsyncGenerator[str, None]:
        """Process streaming events from agent and yield content and hooks."""
        async for chunk in process_streaming_events(result, self.event_handlers):
            yield chunk

    async def stream_message(
        self,
        user_message: str,
        conversation_history: list[dict[str, str]],
        conversation_id: str | None = None,
        user_id: str = "guest.anonymous",
    ) -> AsyncGenerator[str, None]:
        """Stream a message response from the agent with real-time chunks.

        Streams LLM responses in real-time as they are generated, handling:
        - Text deltas from raw LLM responses
        - Complete messages from tool outputs
        - Agent handoffs to sub-agents
        - UI hooks generated by tools
        - Conversation persistence after streaming

        Args:
            user_message: User's message/question
            conversation_history: Previous messages in the conversation
            conversation_id: Cyoda Conversation technical ID
            user_id: User ID from authentication

        Yields:
            Content chunks as they become available, or hook JSON when hooks are available
        """
        try:
            logger.info(
                f"Streaming message for user {user_id}, "
                f"conversation_id={conversation_id}"
            )

            # Step 1: Build full prompt
            full_prompt = self._build_prompt(user_message, conversation_history)

            # Step 2: Create context dict for tools to store hooks
            context = {
                "conversation_id": conversation_id,
                "user_id": user_id,
            }

            # Step 3: Run agent with streaming
            content_gen, accumulated_content = await stream_message(
                self.agent, full_prompt, context, self.event_handlers
            )

            # Step 4: Yield content chunks
            final_accumulated = ""
            async for chunk in content_gen:
                yield chunk
                if not chunk.startswith("{"):
                    final_accumulated += chunk

            # Step 5: Persist conversation after streaming completes
            if conversation_id:
                updated_history = conversation_history.copy()
                updated_history.append({"role": "user", "content": user_message})
                updated_history.append(
                    {"role": "assistant", "content": final_accumulated}
                )
                await self._persist_conversation(
                    conversation_id, updated_history, user_id
                )

            logger.debug(
                f"Streaming completed. Total content: {len(final_accumulated)} chars"
            )

        except Exception as e:
            logger.exception(f"Error streaming message: {e}")
            raise


__all__ = [
    "OpenAIAssistantWrapper",
    "persist_conversation",
    "build_prompt",
    "extract_hooks_from_result",
    "process_message",
    "process_streaming_events",
    "stream_message",
]
