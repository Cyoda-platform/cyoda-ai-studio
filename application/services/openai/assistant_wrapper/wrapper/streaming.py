"""Streaming operations for OpenAI Assistant."""

import importlib
import json
import logging
from typing import Any, AsyncGenerator

from application.config.streaming_config import streaming_config

from ..event_handlers import EventHandlers
from ..constants import (
    EVENT_TYPE_RAW_RESPONSE,
    EVENT_TYPE_RUN_ITEM,
    EVENT_TYPE_AGENT_UPDATED,
)
from .message_processing import extract_hooks_from_result

logger = logging.getLogger(__name__)

# Import OpenAI agents module directly to avoid namespace collision with local 'agents' package
_openai_agents = importlib.import_module("agents")
Agent = _openai_agents.Agent
Runner = _openai_agents.Runner


async def process_streaming_events(
    result: Any, event_handlers: EventHandlers
) -> AsyncGenerator[str, None]:
    """Process streaming events from agent and yield content and hooks.

    Args:
        result: RunResultStreaming from the agent
        event_handlers: Event handlers for processing events

    Yields:
        Content chunks and hook JSON strings
    """
    accumulated_content = ""

    # Step 1: Stream events from the agent
    async for event in result.stream_events():
        logger.debug(f"Streaming event type: {event.type}")

        if event.type == EVENT_TYPE_RAW_RESPONSE:
            accumulated_content, content = event_handlers.handle_raw_response_event(
                event, accumulated_content
            )
            if content:
                yield content

        elif event.type == EVENT_TYPE_RUN_ITEM:
            accumulated_content, content = event_handlers.handle_run_item_event(
                event, accumulated_content
            )
            if content:
                yield content

        elif event.type == EVENT_TYPE_AGENT_UPDATED:
            event_handlers.handle_agent_updated_event(event)

    # Step 2: Add final output
    if result.final_output:
        final_output = str(result.final_output)
        logger.debug(f"Final output: {final_output[:100]}")
        if final_output and final_output not in accumulated_content:
            logger.debug(f"Adding final output: {final_output[:100]}...")
            accumulated_content += final_output
            yield final_output

    # Step 3: Yield hooks as JSON
    hooks = extract_hooks_from_result(result)
    if hooks:
        logger.info(f"Found {len(hooks)} hook(s) in result, yielding as JSON")
        for hook in hooks:
            yield json.dumps({"__hook__": hook})


async def stream_message(
    agent: Agent,
    full_prompt: str,
    context: dict,
    event_handlers: EventHandlers,
) -> tuple[AsyncGenerator[str, None], str]:
    """Stream a message response from the agent with real-time chunks.

    Streams LLM responses in real-time as they are generated, handling:
    - Text deltas from raw LLM responses
    - Complete messages from tool outputs
    - Agent handoffs to sub-agents
    - UI hooks generated by tools

    Args:
        agent: The OpenAI Agent
        full_prompt: Full prompt with conversation history
        context: Context dict for tools
        event_handlers: Event handlers for processing events

    Returns:
        Tuple of (content_generator, accumulated_content)

    Yields:
        Content chunks as they become available, or hook JSON when hooks are available
    """
    logger.debug(f"Running agent in streaming mode: {agent.name}")
    result = Runner.run_streamed(
        agent,
        full_prompt,
        context=context,
        max_turns=streaming_config.MAX_AGENT_TURNS,
    )

    accumulated_content = ""

    async def content_generator():
        nonlocal accumulated_content
        async for chunk in process_streaming_events(result, event_handlers):
            yield chunk
            # Track accumulated content for final persistence
            # Note: hooks are JSON strings starting with {, regular content is plain text
            if not chunk.startswith("{"):
                accumulated_content += chunk

    return content_generator(), accumulated_content
